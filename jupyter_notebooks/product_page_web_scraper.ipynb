{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.softwareadvice.com/categories/')\n",
    "html = response.content\n",
    "soup = bs(html,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_urls = []\n",
    "\n",
    "a_tag_soup = soup('a')\n",
    "\n",
    "for h2s in soup.find_all('h2'):\n",
    "    h2s.decompose()\n",
    "\n",
    "url_lst = []\n",
    "\n",
    "category_url_index_all = -16\n",
    "category_url_index_selection = 13\n",
    "\n",
    "for text in a_tag_soup[10:category_url_index_selection]:   \n",
    "    url = ''\n",
    "    str_text = str(text)[28:]\n",
    "    \n",
    "    for ch in str_text:\n",
    "        if ch != '\"':\n",
    "            url += ch\n",
    "\n",
    "        else:\n",
    "            break   \n",
    " \n",
    "    url += 'p/all/'\n",
    "    if url != 'p/all/':\n",
    "        category_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(category_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in category_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = []\n",
    "\n",
    "for url in category_urls:\n",
    "    response = requests.get(url)\n",
    "#     time.sleep(1)\n",
    "    html = response.content\n",
    "    url_soup = bs(html)\n",
    "\n",
    "    for url in url_soup('a', href=True):\n",
    "        all_urls.append(url['href']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_urls = []\n",
    "\n",
    "for url in all_urls:\n",
    "    if url[-3] == 'l' and url not in product_urls and url[-1] != 'g':\n",
    "        product_urls.append(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(product_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in product_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = []\n",
    "all_avg_reviews = []\n",
    "all_total_reviews = []\n",
    "all_total_recommendations = []\n",
    "all_frontrunner_or_not = []\n",
    "all_overviews = []\n",
    "all_guides_featured_in = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url_scrape_selection = product_urls[:20]\n",
    "product_url_scrape_all = product_urls\n",
    "\n",
    "for i,v in enumerate(product_url_scrape_selection):\n",
    "    while i < len(product_url_scrape_selection):\n",
    "        url = v    \n",
    "        response = requests.get(url)\n",
    "#         time.sleep(1)\n",
    "        html = response.content\n",
    "        soup = bs(html,\"lxml\")\n",
    " \n",
    "\n",
    "        try:\n",
    "            if soup(class_='seo-header'):\n",
    "                seo_header_soup = soup(class_='seo-header')\n",
    "                seo_header_name_lst = [string.get_text(strip=True) for string in seo_header_soup]\n",
    "                seo_header_name = seo_header_name_lst[0]\n",
    "\n",
    "                all_names.append(seo_header_name)\n",
    "            else:\n",
    "\n",
    "                h1_name_soup = soup('h1', class_ = 'bold')\n",
    "                h1_tag_name_list = [string.get_text(strip=True) for string in h1_name_soup]\n",
    "                h1_tag_name = h1_tag_name_list[0]\n",
    "\n",
    "                all_names.append(h1_tag_name)\n",
    "        except:\n",
    "            all_names.append('')\n",
    "        \n",
    "                   \n",
    "        try:\n",
    "            if soup('span', class_ = 'rank-average strong'):\n",
    "                avg_review_soup = soup('span', class_ = 'rank-average strong')\n",
    "                spans = [string.get_text(strip=True) for string in avg_review_soup]\n",
    "                try:\n",
    "                    float(spans[0])\n",
    "                    avg_review = float(spans[0])\n",
    "                    all_avg_reviews.append(avg_review)\n",
    "                except ValueError:\n",
    "                    all_avg_reviews.append(0)         \n",
    "\n",
    "            else:\n",
    "                lead_class_soup = soup(class_='lead')\n",
    "                lead_class_avg_review_lst = [string.get_text(strip=True) for string in lead_class_soup]\n",
    "                avg_review = float(lead_class_avg_review_lst[0])\n",
    "\n",
    "                all_avg_reviews.append(avg_review)\n",
    "        except:\n",
    "            all_avg_reviews.append(0)\n",
    "\n",
    "\n",
    "        a_tag_strings = [string.get_text(strip=True) for string in a_tag_soup]\n",
    "        complete = False\n",
    "\n",
    "        for string in a_tag_strings:\n",
    "            if string[-7:] == 'reviews':\n",
    "                total_reviews = string[:-7]\n",
    "                try:\n",
    "                    int(total_reviews)\n",
    "                    all_total_reviews.append(total_reviews)\n",
    "                    complete = True\n",
    "                    break\n",
    "                except:\n",
    "                    all_total_reviews.append(0)\n",
    "\n",
    "        if complete == False:\n",
    "            for string in a_tag_strings:\n",
    "                    reviews_text = soup('p')\n",
    "                    reviews = [string.get_text(strip=True) for string in reviews_text]\n",
    "                    reviews_string = str(reviews[4:5])\n",
    "                    total_reviews = reviews_string[reviews_string.find('(')+1:reviews_string.find(')')]\n",
    "                    all_total_reviews.append(total_reviews)\n",
    "                    break\n",
    "\n",
    "                 \n",
    "        if soup('p', class_='lead'):\n",
    "            total_recommendations = ''\n",
    "\n",
    "            p_lead_soup = soup('p', class_='lead')\n",
    "            p_lead_strings = str(p_lead_soup[1])\n",
    "            total_rec_ints = [ch for ch in p_lead_strings[23:27] if ch.isdigit()]\n",
    "\n",
    "            for ch in total_rec_ints:\n",
    "                total_recommendations += ch\n",
    "\n",
    "            if len(total_recommendations) > 0:\n",
    "                all_total_recommendations.append(total_recommendations)\n",
    "            else:\n",
    "                all_total_recommendations.append(0)\n",
    "        else:\n",
    "            strong_soup = soup('strong')\n",
    "            strong_strings = [string.get_text(strip=True) for string in strong_soup]\n",
    "            if len(strong_strings) > 0:\n",
    "                total_recommendations_in_last_30_days = strong_strings[0]\n",
    "\n",
    "                if total_recommendations_in_last_30_days.isdigit():\n",
    "                    all_total_recommendations.append(total_recommendations_in_last_30_days)\n",
    "                else:\n",
    "                    all_total_recommendations.append(0) \n",
    "            else:\n",
    "                all_total_recommendations.append(0) \n",
    "            \n",
    "                    \n",
    "        frontrunner_soup = soup('span', class_ = 'ui strong')\n",
    "\n",
    "        if soup(class_='details__frontrunners--tag'):\n",
    "            all_frontrunner_or_not.append('Yes')\n",
    "        else:\n",
    "            if 'FrontRunner' in str(frontrunner_soup):\n",
    "                all_frontrunner_or_not.append('Yes')\n",
    "            else:\n",
    "                all_frontrunner_or_not.append('No')\n",
    "\n",
    "                                     \n",
    "        if soup(id='overview-text'):\n",
    "            overview_soup = soup(id='overview-text')\n",
    "            overview = str(overview_soup)[58:]\n",
    "            all_overviews.append(overview)\n",
    "        else:\n",
    "            overview = ''\n",
    "            overview_soup = soup('p')\n",
    "            overview_str = [string.get_text(strip=True) for string in overview_soup]\n",
    "            for string in overview_str[10:20]:\n",
    "                if len(string) > 30:\n",
    "                    overview += string\n",
    "            \n",
    "            all_overviews.append(overview)\n",
    "            \n",
    "                \n",
    "        if soup('li', class_ ='chips-chip'):\n",
    "            guide_soup = soup('li', class_ ='chips-chip')\n",
    "\n",
    "            guides_featured_in = [string.get_text(strip=True) for string in guide_soup]\n",
    "\n",
    "            all_guides_featured_in.append(guides_featured_in)\n",
    "        else:\n",
    "            all_guides_featured_in.append('None.')\n",
    "            \n",
    "            \n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.Series(all_names)\n",
    "avg_reviews = pd.Series(all_avg_reviews)\n",
    "total_reviews = pd.Series(all_total_reviews)\n",
    "total_recommendations = pd.Series(all_total_recommendations)\n",
    "frontrunner_or_not = pd.Series(all_frontrunner_or_not)\n",
    "overviews = pd.Series(all_overviews)\n",
    "guides_featured_in = pd.Series(all_guides_featured_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = {'Name': names, \n",
    "         'Avg Stars/5': avg_reviews, \n",
    "         'Total Reviews': total_reviews, \n",
    "         'Recommendations': total_recommendations, \n",
    "         'Ever a FrontRunner': frontrunner_or_not,\n",
    "         'Product Overview': overviews,\n",
    "         'Guides Featured In': guides_featured_in}\n",
    "  \n",
    "df = pd.DataFrame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dups = df.drop_duplicates(subset='Name', keep=\"first\")\n",
    "df_no_nan_names = df_no_dups[df_no_dups['Name'].notna()]\n",
    "df_no_nan_names.reset_index(level=0, inplace=True)\n",
    "softwareadvice_product_page_scrape = df_no_nan_names.drop(['index'], axis=1)\n",
    "\n",
    "softwareadvice_product_page_scrape[\"Recommendations\"].replace({\"\": 0}, inplace=True)\n",
    "softwareadvice_product_page_scrape[\"Total Reviews\"].replace({\"\": 0}, inplace=True)\n",
    "softwareadvice_product_page_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softwareadvice_product_page_scrape['Recommendations'] = softwareadvice_product_page_scrape['Recommendations'].astype(int)\n",
    "softwareadvice_product_page_scrape['Total Reviews'] = softwareadvice_product_page_scrape['Total Reviews'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softwareadvice_product_page_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softwareadvice_product_page_scrape.to_csv('softwareadvice_product_page_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mongo_df = pd.read_csv('softwareadvice_product_page_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = 'pymongo_driver_connection_string_here'\n",
    "\n",
    "client =  MongoClient(connection_string)\n",
    "\n",
    "db = client['softare_advice_scrape_db']\n",
    "collection = db['softare_advice_scrape_coll']\n",
    "\n",
    "to_mongo_df.reset_index(inplace=True)\n",
    "to_mongo_df_dict = to_mongo_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.insert_many(to_mongo_df_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
